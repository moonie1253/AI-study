# ResNet 논문

### 문제점
Is learning better networks as easy as stacking more layers? -> vanishing/exploding gradients

Degradation problem
- accuracy saturated -> degrades rapidly
- not caused by overfitting -> higher training error
- not easy to optimize


#### The entire network can sitll be trained end-to-end by SGD
cifar10 으로도 100 layer 넘게 학습 가능

ImageNet에선 더 잘됨 

#### Implementation
- 256x480 resize + horizontal flip -> 224x224로 random crop
1. convolution
2. batch normalization
3. activation(ReLu)
4. initalize weights <- 이것을 안 했기 때문에 정확도가 낮게 나온 것인가
5. use optimizer SGD: learning weight 0.1 시작, 진전 없을 시 10씩 나눔
- weight decay 0.0001 momentum 0.9, no dropout
- batch size 128

#### CIFAR10 의 경우
- 4 pixels padded on each side + horizontal flip, 32x32 random crop

#### Deep plain nets
BN-> neither foward nor backward signals vanish

deep plain nets may have exponentially low convergence rates -> reduce training error

#### Residual nets
use identity mapping for all shortcuts and zero-padding -> no extra params

ResNet-34 exhibits considerably lower training error

when the net is not overly deep, plain도 좋은 성능

대신 ResNet은 빠른 convergence를 제공

### Tip
#### Epoch vs Iteration
Epoch: 전체 데이터 세트를 한 번 돌린 것

Iteration: batch size만큼 돌아간 횟수, len(dataloader)

EX

Image 1000개 batch size 25 epoch 100

총 100 epoch 결과: 1000 이미지 x 100 epoch 횟수 = 100,000 이미지 학습

즉 100,000/25 = 4000 iteration

1 epoch 당 iteration= 이미지 수(1000) / batch size(25) = 40

&nbsp;
&nbsp;

## 왜 Weight Initialize를 해야 하는가?
- optimization 에서 어떤 초기값으로 출발하느냐에 따라 local minimum, global minimum에 빠지는 것 방지
- 아무리 좋은 optimizer 사용해도 초기값 잘못 설정하면 global minimum 으로 수렴하기 어려움
- 거대한 feature space 갖고 있어 올바른 초기값 설정 어려움 -> 정해진 규칙도 없어 적당히 설정
- sigmoid : Xavier init  ReLu : He init 적절  -> pytorch는 함수 내부에서 알아서 해주는 것 같다.
- 초기화하는 방법에 따라서 gradient descent 수렴속도 증가, training error 낮게 학습 가능
