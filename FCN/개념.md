## segmantic segmentaion 세부적 정보 예측까지 하면서 픽셀단위까지 관심을 가짐
### 기존 network 문제점
1. FC layer> no spatial dimension

receptive field 정보를 잃는다.

* receptive field: 출력 레이어의 뉴런 하나에 영향을 미치는 입력 뉴런들의 공간 크기

위그림과 같이 입력이 32x32x3 (RGB image) 인 경우 가중치(필터)의 크기가 [5x5x3] 이라면

receptive field 는 5x5x3 이 된다. 필터의 크기와 같다.

입력이 [16x16x20] 인 경우라면 일반적인 convolution에서 하나의 필터의 크기는 [w x h x 20] 가 되야 한다.

이때 receptive field 는 w x h x 20 이다. 

2. 입력 이미지 크기 고정
- Dense layer 에 가중치 개수 고정, 앞 레이어의 feature map 크기 고정, 연쇄적으로 각 레이어의 feature map 과 imput image
크기 고정


> 한계극복 위해 공간정보를 유지해야 픽셀단위 예측 가능: 1x1 conv활용 

## 구조
- convolutionalization(Dowmsampling)
- Deconvolution(Upsampling)
- skip architecture


&nbsp;

# CODE 해석

|**class**|
|:-------:|
|sky|
|building|
|column/ pole|
|road|
|side walk|
|vegetation|
|traffic light|
|fence|
|vehicle|
|pedestrian|
|bicyclist|
|void|

> 이미 이렇게 class가 분류 되어 있고, 이미 segmentation 되어 있는 이미지를 train 하는 것
>
> 주의점: batch size가 너무 크면 메모리를 잡아 먹어서 shut down 될 수 있음

&nbsp;

## 1. dataset 전처리
resizing the input image and label maps (224,224) -> make them to tensors(여러차원 가진 배열)
> Tensor

![tensor 종류](https://github.com/moonie1253/AI-study/assets/157441976/c6cd88d2-48a4-4a4d-ab12-b273ad7b0afa)


스칼라(0D tensor) 벡터(1D tensor) 행렬(2D tensor)

- 자주 쓰이는 텐서 형태
  - 벡터: 1차원 (특성)
  - 시퀀스: 2차원 (타임스텝, 특성)
  - 이미지: 3차원 (높이, 너비, 채널)
  - 동영상: 4차원 (프레임, 높이, 너비, 채널)

- fibers

  하나의 인덱스를 제외한 나머지 인덱스를 고정할 수 있는 경우 (3-order에서)
  ![image](https://github.com/moonie1253/AI-study/assets/157441976/3e1e16ee-640f-4ea5-8269-7e42b5d19e98)

- slices

  텐서를 2차원 형태의 면적으로 분리
  ![image](https://github.com/moonie1253/AI-study/assets/157441976/8d76c768-3eaa-4815-8a31-5927bf12934a)

&nbsp;

## 2. vgg-16 설계
```python
def block(x, n_convs, filters, kernel_size, activation, pool_size, pool_stride, block_name):
  """
  x(tensor): input image
  n_convs(int): num of conv layers
  filters(ing): num of filters for conv layers
  activation: activation to use in the convolution
  pool_size(int): size of pooling layer
  pool_stride(int): stride of the pooling layer
  block_name(string): name of block 
  """
  for i in range(n_convs):
    x=Conv2D(filters=filters,kernel_size=kernel_size,activation=activation,padding='same',name=f'{block_name}_conv{i+1}')(x)

  x=MaxPooling2D(pool_size=pool_size,strides=pool_stride,name=f'{block_name}_pool{i+1}')(x)

  return x
```

- vgg-16 weight 다운 받기
```python
vgg_weights_path=(경로)
```


- vgg-16 구조

  ![image](https://github.com/moonie1253/AI-study/assets/157441976/d135c978-8757-48a5-aa7e-269f40f5e3ca)

``` python
def VGG_16(image_input):
  #creat 5 blocks(encoder)
  x=block(image_input, n_convs=2, filters=64, kernel_size=(3,3), activation='relu',
          pool_size=(2,2),pool_stride=(2,2), block_name='block1')
  p1=x # (112,112,64) 한 block 끝날 때마다 저장(skip connection 위해 각 pooling layer return

  x=block(x, n_conv=2, filters=128, kernel_size=(3,3), activation='relu',
          pool_size=(2,2), pool_stride=(2,2), blcok_name='block2')
  p2=x # (56,56,128)

  # 이제 3번 반복함
  x=block(x, n_convs=3, filters=256, kernel_size=(3,3), activation='relu',
          pool_size=(2,2), pool_stride=(2,2), block_name='block3')
  p3=x # (28,28,256)

  x=block(x, n_convs=3, filters=512, kernel_size=(3,3), activation='relu',
          pool_size=(2,2), pool_stride=(2,2), block_name='block4')
  p4=x # (14,14,512)

  x=block(x, n_conv=3, filters=512, kernel_size=(3,3), activation='relu',
          pool_size=(2,2), pool_stride=(2,2), block_name='block5')
  p5=x # (7,7,512)  

  # create vgg model
  vgg=Model(image_input, p5) # Model을 만들어줘야 compile,evaluate... 를 할 수 있음, input/output 정의해주면 됨
  vgg.load_weights(vgg_weights_path)

  # extract more features by chaining two more conv layers
  n=4096 # filter/kernel num
  c6=Conv2D(n,(7,7),activation='relu',padding='same',name="conv6")(p5)
  c7=Conv2D(n,(1,1),activation='relu',padding='same',name="conv7")(c6)

  return (p1,p2,p3,p4,c7)
```

&nbsp;

- decoder

  > Conv2DTranspose
  - deconvolution: 원본 input으로 되돌리기 가능
  - transposed convolutional layer: 공간 차원 되돌림 but convolutional 값 되돌리지 못함
    ![images_hayaseleu_post_081aa7f0-68db-4844-a4ac-c402bf7a9d47_1_51F0QJN-0Ra0GzyKCEfsrQ](https://github.com/moonie1253/AI-study/assets/157441976/918be94d-5982-4936-bc2b-6822204a27e2)
  - feature map에서 값 하나 선택(분홍색). 이 값은 스칼라값. 이걸 3x3 filter과 곱함. 출력의 3x3 영역에 그 값 넣기

    즉 transpose convolution에서는 필터와 입력의 '내적'을 계산하는 것이 아닌 입력 값이 필테에 곱해지는 '가중치'역할

    = 필터 x 입력(가중치)
    ![image](https://github.com/moonie1253/AI-study/assets/157441976/83cad5db-6380-478c-baea-4deb58bf23a5)
    stride 로 움직이다 보면 겹치는 부분이 생기는데 그냥 '더해줌'
    ![image](https://github.com/moonie1253/AI-study/assets/157441976/d431ea62-f620-41f9-916a-578ea031d9f3)

```python
def decoder(convs,n_classes):

  f1,f2,f3,f4,f5 = convs # convs=vgg_16 으로 해서 vgg_16의 return값 저장

  # fcn32 kernel size=32 로 해서 바로 upsampling
  fcn32_o= Conv2DTranspose(n_classes,kernel_size=(32,32), strides=(32,32), use_bias=False)(f5)
  fcn32_o= Activation('softmax')(fcn32_o)

  # pool5 upsampling + pool4(1x1 conv해서 크기 맞춰줌)
  o= Conv2DTranspose(n_classes, kernel_size=(4,4), strides=(2,2), use_bias=False)(f5)
  o= Cropping2D(cropping=(1,1))(o)

  o2= f4
  o2= Conv2D(n_classes,(1,1), activation='relu', padding='same')(o2)
  o= tf.keras.layers.Add()([o,o2]) #합침

  # fcn-16 만들기
  fcn16_o= Conv2DTranspose(n_classes, kernel_size=(16,16), strides=(16,16), use_bias=False)(o)
  fcn16_o= Activation('softmax')(fcn16_o)

  # 합친 o를 다시 2x upsampling+ pool3(크기조정 후) 합치기-> 8x upsampling
  o= Conv2DTranspose(n_classes, kernel_size=(4,4), strides=(2,2),use_bias=False)(o)
  o= Cropping2D(cropping=(1,1)(o)

  o2=f3 #pool3
  o2= Conv2D(n_classes, (1,1), activation='relu', padding='same')(o2) # 1x1 conv해서 크기 맞추기
  o= tf.keras.layers.Add()([o,o2]) # 8x upsampling 하기 전 합치기

  # fcn-8 만들기
  fcn8_o= Conv2DTranspose(n_classes, kernel_size=(8,8), strides=(8,8), use_bias=False)(o)
  fcn8_o= Activation('softmax')(o)

  return fcn32_o, fcn16_o, fcn8_o

```
![image](https://github.com/moonie1253/AI-study/assets/157441976/5c268599-fdde-48b5-a638-3e67527239fe)
&nbsp;

2x upsampling 된 부분은 (4,4) kernel에 (2,2) strides로 Conv2DTranspose한 다음 crop을 한다.

&nbsp;

- encoder & decoder 합치기

```python
def segmentation_model():
  inputs=Input(shape=(224,224,3))
  convs= VGG_16(inputs)
  fcn32, fcn16, fcn8 =decoder(convs, 12) # 12 classes
  model_fcn32= Model(inputs, fcn32)
  model_fcn16= Model(inputs, fcn16)
  model_fcn8= Model(inputs, fcn8)

  return model_fcn32, model_fcn16, model_fcn8

model_fcn32, model_fcn16, model_fcn8 = segmentation_model()
```

&nbsp;

## Compile the model

``` python
adam= tf.keras.optimizers.Adam(learning_rate=0.001)
model_fcn32.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])
model_fcn16.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])
model_fcn8.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])
```
&nbsp;

## Train the model
```python
# num of training images
train_count= len(training_image_paths)

# num of validation images
valid_count= len(validation_image_paths)

Epochs =170
BATCH_SIZE=64

steps_per_epoch= train_count//BATCH_SIZE
validation_steps= valid_count//BATCH_SIZE

# fcn32
history_fcn32= model_fcn32.fit(training_dataset, steps_per_epoch=steps_per_epoch,
                                validation_data=validation_dataset,
                                validation_steps=validation_steps,
                                epochs=100)

# fcn16
history_fcn16= model_fcn16.fit(training_dataset, steps_per_epoch=steps_per_epoch,
                                validation_data=validation_dataset
                                validation_steps=validation_steps,
                                epochs=EPOCHS)

# fcn8
history_fcn8= model_fcn8.fit(training_dataset, steps_per_epoch=steps_per_epoch,
                              validation_data=validation_dataset,
                              validation_steps=validation_steps,
                              epochs=EPOCHS)
```
  


