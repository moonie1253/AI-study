{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d735d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# 주로 사용하는 코드 2 : 인식한 GPU 개수 출력\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0ef32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49897e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./datasets_tf/creditcard.csv', na_values='#NAME?')\n",
    "# 데이터 파일에서 #NAME? 값이 있으면 결즉치(값 비어있거나 존재하지 않는 경우)로 처리\n",
    "# 앞에 점을 찍어줘야 현재 작업 디렉토리 라는 뜻으로 이해 가능함\n",
    "\n",
    "x=df[['V17','V9','V6','V12']]\n",
    "y=df['Class']\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3, random_state=101)\n",
    "#random state은 동일 시드를 생성하는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7dacc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['entropy', 'gini'],\n",
       "                                        'max_depth': [2],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [4, 6, 8],\n",
       "                                        'min_samples_split': [5, 7, 10],\n",
       "                                        'n_estimators': [20]},\n",
       "                   random_state=101, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random search\n",
    "# 구간 정해주고 그 구간 안 랜덤으로 숫자 뽑아서 실험\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#트리의 노드를 반복적으로 분할\n",
    "\n",
    "\"\"\"\n",
    "criterion: 노드 분할 기준 정하는 매개변수, entropy/ gini 중 택일\n",
    "max_depth: 트리의 최대깊이 제한 과적합 방지\n",
    "max_features: 특성 개수의 제곱근 만큼 특성 선택\n",
    "min_samples_leaf: 리프 노드가 되기 위한 최소 샘플 수 지정, 이 값보다 작으면 더이상\n",
    "                    분할하지 않고 리프 노드가 된다\n",
    "min_samples_split: 노드를 분할하기 위한 최소 샘플 수 지정, 이 값보다 작으면 더이상 \n",
    "                    분할 하지 않고 리프 노드 됨\n",
    "n_estimators: 생성할 트리의 개수 지정\"\"\"\n",
    "\n",
    "random_search={'criterion':['entropy','gini'],\n",
    "              'max_depth':[2],\n",
    "              'max_features':['auto','sqrt'],\n",
    "              'min_samples_leaf':[4,6,8],\n",
    "              'min_samples_split':[5,7,10],\n",
    "              'n_estimators':[20]}\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "model=RandomizedSearchCV(estimator=clf,param_distributions=random_search,n_iter=10,\n",
    "                        cv=4, verbose=1,random_state=101,n_jobs=-1)\n",
    "\"\"\"\n",
    "estimator: 탐색할 모델\n",
    "param_distributions: 탐색할 하이퍼파라미터 공간, 이 공간에서 무작위 샘플링\n",
    "n_iter: 랜덤 탐색에서 시도할 하이퍼파라미터 조합의 수\n",
    "cv: 교차검정을 수행할 폴드 수\n",
    "n_jobs: 적합성과 예측성을 위해 병렬로 실행할 작업 수\n",
    "        병렬 처리에 사용할 cpu 코어의 수\n",
    "        -1로 설정 시 가능한 모든 코어 사용\"\"\"\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "500d2dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 20, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 2, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba661ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85299\n",
      "           1       0.84      0.56      0.67       144\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.92      0.78      0.83     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "\n",
      "Random Search:  0.9990754069964772\n"
     ]
    }
   ],
   "source": [
    "randompredict=model.best_estimator_.predict(x_test)\n",
    "print(classification_report(y_test,randompredict))\n",
    "print(\"\\nRandom Search: \", accuracy_score(y_test,randompredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82478779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 36 candidates, totalling 144 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['entropy', 'gini'], 'max_depth': [2],\n",
       "                         'max_features': ['auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [4, 6, 8],\n",
       "                         'min_samples_split': [5, 7, 10],\n",
       "                         'n_estimators': [20]},\n",
       "             verbose=5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search\n",
    "\"\"\"\n",
    "random search와 차이점\n",
    "random: 탐색 범위 주어지고 해당 범위 내 임의 조합 찾아 탐색\n",
    "grid: 탐색할 값이 주어지고 모든 조합 탐색\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search={'criterion':['entropy','gini'],\n",
    "            'max_depth':[2],\n",
    "            'max_features':['auto','sqrt'],\n",
    "            'min_samples_leaf':[4,6,8],\n",
    "            'min_samples_split':[5,7,10],\n",
    "            'n_estimators':[20]}\n",
    "clf=RandomForestClassifier()\n",
    "model=GridSearchCV(estimator=clf, param_grid=grid_search,cv=4,verbose=5,n_jobs=-1)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7268f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 2, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2bda45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85299\n",
      "           1       0.83      0.69      0.76       144\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.92      0.85      0.88     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "\n",
      "Grid Search:  0.9992509626300574\n"
     ]
    }
   ],
   "source": [
    "gridpredict=model.best_estimator_.predict(x_test)\n",
    "print(classification_report(y_test,gridpredict))\n",
    "print(\"\\nGrid Search: \",accuracy_score(y_test,gridpredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f1b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from hyperopt) (1.21.5)\n",
      "Requirement already satisfied: six in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from hyperopt) (1.7.3)\n",
      "Collecting future\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     ------------------------------------- 840.9/840.9 kB 13.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     -------------------------------------- 200.5/200.5 kB 6.1 MB/s eta 0:00:00\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 11.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492055 sha256=24695b4636231947cf6483ed6a7e6e98e03b4fbf168cdb8527c3197fcee56e93\n",
      "  Stored in directory: c:\\users\\tlsdu\\appdata\\local\\pip\\cache\\wheels\\52\\2a\\fc\\520209cfa6448febd490720a0b09036cb367628f7c4e9cc172\n",
      "Successfully built future\n",
      "Installing collected packages: py4j, tqdm, networkx, future, cloudpickle, hyperopt\n",
      "Successfully installed cloudpickle-2.2.1 future-0.18.3 hyperopt-0.2.7 networkx-2.6.3 py4j-0.10.9.7 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "# Bayesian Optimization\n",
    "\"\"\"\n",
    "1. 관측 데이터 기반 목적 함수를 추정: Gaussian Process\n",
    "2. 추정 모델 기반 탐색한 파라미터 선택: Acquisition Function\n",
    "-평균이 최대\n",
    "-분산이 최대\n",
    "3. 다음 관측 데이터에 추가\n",
    "\n",
    "목적함수와 하이퍼 파라미터 조합을 대상으로 평가 후 관측 데이터에 추가하는 과정을 \n",
    "반복하면서 순차적으로 업데이트 하여 최적의 조합을 탐색하는 방법\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "입력값 x 받는 미지의 목적 함수 f(x) 상정 후 해당 함숫값을 최대로 만드는 최적해를\n",
    "찾는 것을 목적으로 한다. \n",
    "두가지 필수 요소 존재\n",
    "1. Surrogate Model은 현재까지 조사된 입력값-함숫값 점 바탕으로 미지의 목적 함수 형태\n",
    "에 대한 확률적인 추정을 수행하는 모델\n",
    "2. Acquisition Function은 목적 함수에 대한 현재까지의 확률적 추정 결과를 바탕으로\n",
    "최적 입력값을 찾는 데 있어 가장 유용할 만한 다음 입력값 후보를 추천해 주는 함수 지칭\n",
    "\"\"\"\n",
    "\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bd48121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in          \n",
      "database. History logging moved to new session        \n",
      "209                                                   \n",
      "100%|██████████| 20/20 [01:53<00:00,  5.66s/trial, best loss: 0.9982544491482915]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 0,\n",
       " 'max_depth': 10.0,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 0.10591490048784225,\n",
       " 'min_samples_split': 0.21029259237553422,\n",
       " 'n_estimators': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "\n",
    "space={'criterion':hp.choice('criterion',['entropy','gini']),\n",
    "      'max_depth':hp.quniform('max_depth',10,12,10),\n",
    "      'max_features':hp.choice('max_features',['auto','sqrt','log2',None]),\n",
    "      'min_samples_leaf':hp.uniform('min_samples_leaf',0,0.5),\n",
    "      'min_samples_split':hp.uniform('min_samples_split',0,1),\n",
    "      'n_estimators':hp.choice('n_estimators',[10,50])}\n",
    "#hp.quniform 은 실수 값 반환하지만 주어진 간격에 따라 양자화된다\n",
    "# 10 부터 12까지의 값 균일하게 샘플링 하지만 반환된 값은 10,11,12 중 하나일 것이다.\n",
    "\n",
    "def objective(space):\n",
    "    hopt=RandomForestClassifier(criterion=space['criterion'],\n",
    "                               max_depth=space['max_depth'],\n",
    "                               max_features=space['max_features'],\n",
    "                               min_samples_leaf=space['min_samples_leaf'],\n",
    "                               min_samples_split=space['min_samples_split'],\n",
    "                               n_estimators=space['n_estimators'])\n",
    "    accuracy=cross_val_score(hopt,x_train,y_train,cv=4).mean()\n",
    "    return {'loss':accuracy,'status':STATUS_OK}\n",
    "\n",
    "trials=Trials()\n",
    "best=fmin(fn=objective,\n",
    "         space=space,\n",
    "         algo=tpe.suggest,\n",
    "         max_evals=20,\n",
    "         trials=trials)\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f74ebd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85299\n",
      "           1       0.00      0.00      0.00       144\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.50      0.50      0.50     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "crit={0:'entropy',1:'gini'}\n",
    "feat={0:'auto',1:'sqrt',2:'log2',3:None}\n",
    "est={0:10,1:50,2:75,3:100,4:125}\n",
    "\n",
    "trainedforest=RandomForestClassifier(criterion=crit[best['criterion']],\n",
    "                                    max_depth=best['max_depth'],\n",
    "                                    max_features=feat[best['max_features']],\n",
    "                                    min_samples_leaf=best['min_samples_leaf'],\n",
    "                                    min_samples_split=best['min_samples_split'],\n",
    "                                    n_estimators=est[best['n_estimators']]\n",
    "                                    ).fit(x_train,y_train)\n",
    "\n",
    "predictionforest=trainedforest.predict(x_test)\n",
    "print(classification_report(y_test,predictionforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6509f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Optimization:  0.9983146659176293\n"
     ]
    }
   ],
   "source": [
    "print(\"Bayesian Optimization: \",accuracy_score(y_test,predictionforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bfe1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "     -------------------------------------- 413.4/413.4 kB 8.6 MB/s eta 0:00:00\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "     ------------------------------------- 226.8/226.8 kB 14.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from optuna) (1.21.5)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-2.0.25-cp37-cp37m-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 11.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from optuna) (22.0)\n",
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0.1-cp37-cp37m-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.2/153.2 kB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.3)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from alembic>=1.5.0->optuna) (5.2.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.7/78.7 kB ? eta 0:00:00\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp37-cp37m-win_amd64.whl (291 kB)\n",
      "     ------------------------------------- 291.4/291.4 kB 17.6 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\tlsdu\\anaconda3\\envs\\mine\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: typing-extensions, PyYAML, greenlet, colorlog, sqlalchemy, Mako, alembic, optuna\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed Mako-1.2.4 PyYAML-6.0.1 alembic-1.12.1 colorlog-6.8.2 greenlet-3.0.3 optuna-3.5.0 sqlalchemy-2.0.25 typing-extensions-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9312574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "\n",
    "def objective(trial):\n",
    "    iris=sklearn.datasets.load_iris()\n",
    "    x,y=iris.dat,iris.target\n",
    "    \n",
    "    classifier_name=trai.suggest_categorical('classifier',['SVC','RandomForest'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
